{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id      target\n",
      "count   7613.000000  7613.00000\n",
      "mean    5441.934848     0.42966\n",
      "std     3137.116090     0.49506\n",
      "min        1.000000     0.00000\n",
      "25%     2734.000000     0.00000\n",
      "50%     5408.000000     0.00000\n",
      "75%     8146.000000     1.00000\n",
      "max    10873.000000     1.00000\n",
      "   id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "print(df_train.describe())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What's up man?\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['target'] == 0]['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train['target'] == 1]['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'\n",
      " 'Forest fire near La Ronge Sask. Canada'\n",
      " \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      " ... 'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ'\n",
      " 'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.'\n",
      " 'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d']\n"
     ]
    }
   ],
   "source": [
    "print(df_train['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 54)\n",
      "[[0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# let's get counts for the first 5 tweets in the data\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "example_train_vectors = count_vectorizer.fit_transform(df_train[\"text\"][0:5])\n",
    "\n",
    "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 21637)\n",
      "(1, 21637)\n"
     ]
    }
   ],
   "source": [
    "# let's get counts for all tweets in the data\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(df_train['text'])\n",
    "test_vectors = count_vectorizer.transform(df_test['text'])\n",
    "\n",
    "print(train_vectors[0].todense().shape)\n",
    "print(test_vectors[0].todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      1\n",
      "16      0\n",
      "17      1\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      1\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      1\n",
      "       ..\n",
      "3233    1\n",
      "3234    1\n",
      "3235    1\n",
      "3236    1\n",
      "3237    1\n",
      "3238    1\n",
      "3239    1\n",
      "3240    1\n",
      "3241    0\n",
      "3242    0\n",
      "3243    1\n",
      "3244    0\n",
      "3245    0\n",
      "3246    0\n",
      "3247    0\n",
      "3248    0\n",
      "3249    0\n",
      "3250    0\n",
      "3251    0\n",
      "3252    1\n",
      "3253    1\n",
      "3254    1\n",
      "3255    1\n",
      "3256    1\n",
      "3257    1\n",
      "3258    1\n",
      "3259    1\n",
      "3260    1\n",
      "3261    1\n",
      "3262    0\n",
      "Name: target, Length: 3263, dtype: int64\n",
      "count    7613.00000\n",
      "mean        0.42966\n",
      "std         0.49506\n",
      "min         0.00000\n",
      "25%         0.00000\n",
      "50%         0.00000\n",
      "75%         1.00000\n",
      "max         1.00000\n",
      "Name: target, dtype: float64\n",
      "             0            1            2            3            4      \\\n",
      "count  7613.000000  7613.000000  7613.000000  7613.000000  7613.000000   \n",
      "mean      0.004335     0.000525     0.000131     0.000131     0.000131   \n",
      "std       0.075035     0.022917     0.011461     0.011461     0.011461   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       2.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "             5            6            7            8            9      \\\n",
      "count  7613.000000  7613.000000  7613.000000  7613.000000  7613.000000   \n",
      "mean      0.000131     0.000131     0.003415     0.000919     0.000131   \n",
      "std       0.011461     0.011461     0.084157     0.030311     0.011461   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     3.000000     1.000000     1.000000   \n",
      "\n",
      "          ...             21627        21628        21629        21630  \\\n",
      "count     ...       7613.000000  7613.000000  7613.000000  7613.000000   \n",
      "mean      ...          0.005517     0.000131     0.000131     0.000131   \n",
      "std       ...          0.077541     0.011461     0.011461     0.011461   \n",
      "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "max       ...          2.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "             21631        21632        21633        21634        21635  \\\n",
      "count  7613.000000  7613.000000  7613.000000  7613.000000  7613.000000   \n",
      "mean      0.003678     0.000131     0.000131     0.000131     0.000131   \n",
      "std       0.060538     0.011461     0.011461     0.011461     0.011461   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "             21636  \n",
      "count  7613.000000  \n",
      "mean      0.000131  \n",
      "std       0.011461  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max       1.000000  \n",
      "\n",
      "[8 rows x 21637 columns]\n"
     ]
    }
   ],
   "source": [
    "print(sample_submission['target'])\n",
    "print(df_train['target'].describe())\n",
    "print(pd.DataFrame(train_vectors).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "print(xgboost.__version__)\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = XGBClassifier(objective=\"reg:squarederror\",\n",
    "                    max_depth=5,\n",
    "                    min_child_weight=1,\n",
    "                    gamma=0,\n",
    "                    subsample = 0.8,\n",
    "                    colsample_bytree = 0.8,\n",
    "                    use_label_encoder=False,\n",
    "                    scale_pos_weight = 1,\n",
    "                    cv=3,\n",
    "                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.59610028 0.54219949 0.63914522]\n"
     ]
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(XGBClassifier(objective='reg:squarederror', use_label_encoder=False,), train_vectors, df_train['target'], cv=3, scoring='f1')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:08] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { cv } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make submission\n",
    "classifier.fit(train_vectors, df_train[\"target\"])\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "sample_submission['target'] = classifier.predict(test_vectors)\n",
    "sample_submission.head()\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "5       0\n",
      "6       0\n",
      "7       0\n",
      "8       0\n",
      "9       0\n",
      "10      0\n",
      "11      0\n",
      "12      0\n",
      "13      0\n",
      "14      0\n",
      "15      1\n",
      "16      0\n",
      "17      1\n",
      "18      0\n",
      "19      0\n",
      "20      0\n",
      "21      0\n",
      "22      0\n",
      "23      1\n",
      "24      0\n",
      "25      0\n",
      "26      0\n",
      "27      0\n",
      "28      0\n",
      "29      1\n",
      "       ..\n",
      "3233    1\n",
      "3234    1\n",
      "3235    1\n",
      "3236    1\n",
      "3237    1\n",
      "3238    1\n",
      "3239    1\n",
      "3240    1\n",
      "3241    0\n",
      "3242    0\n",
      "3243    1\n",
      "3244    0\n",
      "3245    0\n",
      "3246    0\n",
      "3247    0\n",
      "3248    0\n",
      "3249    0\n",
      "3250    0\n",
      "3251    0\n",
      "3252    1\n",
      "3253    1\n",
      "3254    1\n",
      "3255    1\n",
      "3256    1\n",
      "3257    1\n",
      "3258    1\n",
      "3259    1\n",
      "3260    1\n",
      "3261    1\n",
      "3262    0\n",
      "Name: target, Length: 3263, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sample_submission['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, learning_curve, validation_curve\n",
    "from sklearn.model_selection import cross_validate, RandomizedSearchCV, GridSearchCV, KFold\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Learning curves\\ntrain_sizes, train_scores, test_scores = create_learning_curve(\\n    df, pl_classifier, scoring, rs_cv, X_train, y_train)\\n\\nplot_learning_curve(train_sizes, train_scores, test_scores, 'Learning Curve', alpha=0.1)\\n\\n# Model complexity curves\\n# parameter: max_depth\\nparam_name = 'classifier__max_depth'\\nparam_range = np.arange(3, 30, 3)\\ntrain_scores, test_scores = create_validation_curve(\\n    df, pl_classifier, scoring, rs_cv, X_train, y_train, param_name, param_range)\\n\\nplot_validation_curve(param_range, train_scores, test_scores, param_name, alpha=0.1)\\n\\n# parameter: min_samples_leaf\\nparam_name = 'classifier__min_samples_leaf'\\nparam_range = np.arange(1, 31, 5)\\ntrain_scores, test_scores = create_validation_curve(\\n    df, pl_classifier, scoring, rs_cv, X_train, y_train, param_name, param_range)\\n\\nplot_validation_curve(param_range, train_scores, test_scores, param_name, alpha=0.1)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assumption: max_depth and max_leaf_nodes are somewhat redundant as pruning techniques\n",
    "# Reference: https://datascience.stackexchange.com/questions/29520/how-to-plot-learning-curve-and-validation-curve-while-using-pipeline\n",
    "def find_best_estimator(estimator, scoring, X_train, X_test, y_train, y_test, **parameters):\n",
    "    # Search for near-optimal hyperparameters\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    rs_cv = RandomizedSearchCV(estimator=estimator, param_distributions=parameters, \n",
    "                                scoring=scoring, n_jobs=5, refit=True, cv=cv, verbose=1, \n",
    "                                random_state=0, return_train_score=True)\n",
    "    rs_cv = rs_cv.fit(X_train, y_train)\n",
    "    print('Tuned randomized search best parameters: {}'.format(rs_cv.best_params_))\n",
    "\n",
    "    print('Training Report')\n",
    "    ypred = rs_cv.predict(X_train)\n",
    "    print(classification_report(y_train, ypred))\n",
    "    print('\\n')\n",
    "    print('Testing Report')\n",
    "    ypred2 = rs_cv.predict(X_test)\n",
    "    print(classification_report(y_test, ypred2))\n",
    "\n",
    "    return rs_cv, X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_learning_curve(scoring, s_cv, X_train, y_train):\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator=s_cv.best_estimator_, X=X_train, y=y_train, random_state=0,\n",
    "        train_sizes=np.arange(0.05, 1.05, 0.1), cv=cv, scoring=scoring, n_jobs=2)\n",
    "    \n",
    "    return train_sizes, train_scores, test_scores\n",
    "\n",
    "def create_validation_curve(scoring, s_cv, X_train, y_train, param_name, param_range):\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator=s_cv.best_estimator_, X=X_train, y=y_train, \n",
    "        param_name=param_name, param_range=param_range, cv=cv, scoring=scoring, n_jobs=2)\n",
    "\n",
    "    return train_scores, test_scores\n",
    "    \n",
    "def plot_learning_curve(train_sizes, train_scores, test_scores, title, alpha=0.1):\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    test_std = np.std(test_scores, axis=1)\n",
    "    plt.plot(train_sizes, train_mean, label='train score', color='blue', marker='o')\n",
    "    plt.fill_between(train_sizes, train_mean + train_std,\n",
    "                     train_mean - train_std, color='blue', alpha=alpha)\n",
    "    plt.plot(train_sizes, test_mean, label='test score', color='red', marker='o')\n",
    "\n",
    "    plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, color='red', alpha=alpha)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Number of training points')\n",
    "    plt.ylabel('F1')\n",
    "    plt.grid(ls='--')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "def plot_validation_curve(param_range, train_scores, test_scores, title, alpha=0.1):\n",
    "    sort_idx = np.argsort(param_range)\n",
    "    param_range=np.array(param_range)[sort_idx]\n",
    "    train_mean = np.mean(train_scores, axis=1)[sort_idx]\n",
    "    train_std = np.std(train_scores, axis=1)[sort_idx]\n",
    "    test_mean = np.mean(test_scores, axis=1)[sort_idx]\n",
    "    test_std = np.std(test_scores, axis=1)[sort_idx]\n",
    "    plt.plot(param_range, train_mean, label='train score', color='blue', marker='o')\n",
    "    plt.fill_between(param_range, train_mean + train_std,\n",
    "                 train_mean - train_std, color='blue', alpha=alpha)\n",
    "    plt.plot(param_range, test_mean, label='test score', color='red', marker='o')\n",
    "    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, color='red', alpha=alpha)\n",
    "    plt.title(title)\n",
    "    plt.grid(ls='--')\n",
    "    plt.xlabel('Parameter Value')\n",
    "    plt.ylabel('Average values and standard deviation')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "# Learning curves\n",
    "train_sizes, train_scores, test_scores = create_learning_curve(\n",
    "    df, pl_classifier, scoring, rs_cv, X_train, y_train)\n",
    "\n",
    "plot_learning_curve(train_sizes, train_scores, test_scores, 'Learning Curve', alpha=0.1)\n",
    "\n",
    "# Model complexity curves\n",
    "# parameter: max_depth\n",
    "param_name = 'classifier__max_depth'\n",
    "param_range = np.arange(3, 30, 3)\n",
    "train_scores, test_scores = create_validation_curve(\n",
    "    df, pl_classifier, scoring, rs_cv, X_train, y_train, param_name, param_range)\n",
    "\n",
    "plot_validation_curve(param_range, train_scores, test_scores, param_name, alpha=0.1)\n",
    "\n",
    "# parameter: min_samples_leaf\n",
    "param_name = 'classifier__min_samples_leaf'\n",
    "param_range = np.arange(1, 31, 5)\n",
    "train_scores, test_scores = create_validation_curve(\n",
    "    df, pl_classifier, scoring, rs_cv, X_train, y_train, param_name, param_range)\n",
    "\n",
    "plot_validation_curve(param_range, train_scores, test_scores, param_name, alpha=0.1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
       "       colsample_bynode=None, colsample_bytree=0.8, gamma=0, gpu_id=None,\n",
       "       importance_type='gain', interaction_constraints=None,\n",
       "       learning_rate=0.1, max_delta_step=None, max_depth=5,\n",
       "       min_child_weight=1, missing=nan, monotone_constraints=None,\n",
       "       n_estimators=509, n_jobs=None, nthread=4, num_parallel_tree=None,\n",
       "       objective='reg:squarederror', random_state=None, reg_alpha=None,\n",
       "       reg_lambda=None, scale_pos_weight=1, seed=27, subsample=0.8,\n",
       "       tree_method=None, use_label_encoder=True, validate_parameters=None,\n",
       "       verbosity=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split training and testing data\n",
    "X = train_vectors\n",
    "y = df_train[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=10)\n",
    "\n",
    "# Find an initial optimal n_estimators\n",
    "classifier = XGBClassifier(\n",
    "             learning_rate=0.1,\n",
    "             n_estimators=1000,\n",
    "             max_depth=5,\n",
    "             min_child_weight=1,\n",
    "             gamma=0,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=0.8,\n",
    "             objective= 'reg:squarederror',\n",
    "             nthread=4,\n",
    "             scale_pos_weight=1,\n",
    "             seed=27)\n",
    "xgb_param = classifier.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X, label=y)\n",
    "cvresult = xgboost.cv(xgb_param, xgtrain, num_boost_round=classifier.get_params()['n_estimators'], nfold=5,\n",
    "            metrics='rmse', early_stopping_rounds=50)\n",
    "classifier.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-4b37fb1773b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m rs_cv, X_train, X_test, y_train, y_test = find_best_estimator(\n\u001b[0;32m---> 12\u001b[0;31m     classifier, scoring, X_train, X_test, y_train, y_test, **tree_parameters)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-9c37c2e4f347>\u001b[0m in \u001b[0;36mfind_best_estimator\u001b[0;34m(estimator, scoring, X_train, X_test, y_train, y_test, **parameters)\u001b[0m\n\u001b[1;32m      8\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                 random_state=0, return_train_score=True)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mrs_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tuned randomized search best parameters: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         evaluate_candidates(ParameterSampler(\n\u001b[1;32m   1514\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m             random_state=self.random_state))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep37/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tune tree specific parameters\n",
    "tree_parameters = { 'max_depth': [3, 5, 10],\n",
    "                    'min_child_weight': [0.1, 1, 5],\n",
    "                    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "                    'subsample': [0.5, 0.75, 1.0],\n",
    "                    'colsample_bytree': [0.5, 0.75, 1.0]\n",
    "                  }\n",
    "\n",
    "scoring = make_scorer(f1_score)\n",
    "\n",
    "rs_cv, X_train, X_test, y_train, y_test = find_best_estimator(\n",
    "    classifier, scoring, X_train, X_test, y_train, y_test, **tree_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned randomized search best parameters: {'subsample': 0.75, 'min_child_weight': 0.1, 'max_depth': 3, 'gamma': 1, 'colsample_bytree': 1.0}\n",
    "# Reduce min_child_weight from 1 to 0.1 reduces underfitting\n",
    "# Reduce max_depth from 5 to 3 reduces overfitting\n",
    "# Increase gamma from 0 to 1 requires more improvement for a split\n",
    "# colsample_bytree from 0.8 to 1 requires all columns\n",
    "# BEST SUBMISSION FOR XG BOOST - LATER ONES HAVE WORSE PERFORMANCE\n",
    "classifier = XGBClassifier(\n",
    "             learning_rate=0.1,\n",
    "             n_estimators=509,\n",
    "             max_depth=3,\n",
    "             min_child_weight=0.1,\n",
    "             gamma=1,\n",
    "             subsample=0.75,\n",
    "             colsample_bytree=1.0,\n",
    "             objective= 'reg:squarederror',\n",
    "             nthread=4,\n",
    "             scale_pos_weight=1,\n",
    "             seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "classifier.fit(train_vectors, df_train[\"target\"])\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "sample_submission['target'] = classifier.predict(test_vectors)\n",
    "sample_submission.head()\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=5)]: Done  50 out of  50 | elapsed: 23.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned randomized search best parameters: {'subsample': 0.8, 'min_child_weight': 0.1, 'max_depth': 3, 'gamma': 0.8, 'colsample_bytree': 1.0}\n",
      "Training Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88      3711\n",
      "           1       0.90      0.74      0.81      2760\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6471\n",
      "   macro avg       0.86      0.84      0.84      6471\n",
      "weighted avg       0.86      0.85      0.85      6471\n",
      "\n",
      "\n",
      "\n",
      "Testing Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.82       631\n",
      "           1       0.84      0.66      0.74       511\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1142\n",
      "   macro avg       0.80      0.78      0.78      1142\n",
      "weighted avg       0.80      0.79      0.79      1142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune tree specific parameters\n",
    "tree_parameters = { 'max_depth': [2, 3, 4],\n",
    "                    'min_child_weight': [0, 0.1, 0.2],\n",
    "                    'gamma': [0.8, 1, 1.2],\n",
    "                    'subsample': [0.7, 0.8, 0.9],\n",
    "                    'colsample_bytree': [1.0]\n",
    "                  }\n",
    "\n",
    "scoring = make_scorer(f1_score)\n",
    "\n",
    "rs_cv, X_train, X_test, y_train, y_test = find_best_estimator(\n",
    "    classifier, scoring, X_train, X_test, y_train, y_test, **tree_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned randomized search best parameters: {'subsample': 0.8, 'min_child_weight': 0.1, 'max_depth': 3, 'gamma': 0.8, 'colsample_bytree': 1.0}\n",
    "classifier = XGBClassifier(\n",
    "             learning_rate=0.1,\n",
    "             n_estimators=736,\n",
    "             max_depth=3,\n",
    "             min_child_weight=0.1,\n",
    "             gamma=0.8,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=1.0,\n",
    "             objective= 'reg:squarederror',\n",
    "             nthread=4,\n",
    "             scale_pos_weight=1,\n",
    "             seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1.0, gamma=0.8, gpu_id=-1,\n",
       "       importance_type='gain', interaction_constraints='',\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=0.1, missing=nan, monotone_constraints='()',\n",
       "       n_estimators=736, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
       "       objective='reg:squarederror', random_state=27, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=27, subsample=0.8,\n",
       "       tree_method='exact', use_label_encoder=True, validate_parameters=1,\n",
       "       verbosity=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find new optimal number of estimators\n",
    "xgb_param = classifier.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X, label=y)\n",
    "cvresult = xgboost.cv(xgb_param, xgtrain, num_boost_round=classifier.get_params()['n_estimators'], nfold=5,\n",
    "            metrics='rmse', early_stopping_rounds=50)\n",
    "classifier.set_params(n_estimators=cvresult.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "classifier.fit(train_vectors, df_train[\"target\"])\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "sample_submission['target'] = classifier.predict(test_vectors)\n",
    "sample_submission.head()\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/anaconda3/envs/deep37/lib/python3.7/site-packages/sklearn/model_selection/_search.py:271: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done  25 out of  25 | elapsed: 16.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned randomized search best parameters: {'alpha': 1e-05}\n",
      "Training Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.88      3711\n",
      "           1       0.90      0.75      0.82      2760\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6471\n",
      "   macro avg       0.87      0.84      0.85      6471\n",
      "weighted avg       0.86      0.86      0.86      6471\n",
      "\n",
      "\n",
      "\n",
      "Testing Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.89      0.82       631\n",
      "           1       0.83      0.67      0.74       511\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1142\n",
      "   macro avg       0.80      0.78      0.78      1142\n",
      "weighted avg       0.80      0.79      0.79      1142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tune alpha\n",
    "# Tune tree specific parameters\n",
    "tree_parameters = { 'alpha': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "                  }\n",
    "\n",
    "scoring = make_scorer(f1_score)\n",
    "\n",
    "rs_cv, X_train, X_test, y_train, y_test = find_best_estimator(\n",
    "    classifier, scoring, X_train, X_test, y_train, y_test, **tree_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate and add trees\n",
    "#Tuned randomized search best parameters: {'subsample': 0.75, 'min_child_weight': 0.1, 'max_depth': 3, 'gamma': 1, 'colsample_bytree': 1.0}\n",
    "classifier = XGBClassifier(\n",
    "             learning_rate=0.01,\n",
    "             n_estimators=2500,\n",
    "             max_depth=3,\n",
    "             min_child_weight=0.1,\n",
    "             gamma=0.8,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=1.0,\n",
    "             alpha=1e-5,\n",
    "             objective= 'reg:squarederror',\n",
    "             nthread=4,\n",
    "             scale_pos_weight=1,\n",
    "             seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make submission\n",
    "classifier.fit(train_vectors, df_train[\"target\"])\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "sample_submission['target'] = classifier.predict(test_vectors)\n",
    "sample_submission.head()\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
